wandb_version: 1

base_path:
  desc: null
  value: /home/marcin.osial/IntervMerge/src
pretrained_checkpoint:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/checkpoints/ViT-B-32/zeroshot.pt
deterministic:
  desc: null
  value: true
batch_size:
  desc: null
  value: 128
cache_dir:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/cache_dir
data_location:
  desc: null
  value: /home/marcin.osial/AdaMerging/data
exam_datasets:
  desc: null
  value:
  - SUN397
  - Cars
  - RESISC45
  - EuroSAT
  - SVHN
  - GTSRB
  - MNIST
  - DTD
save_checkpoints:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/checkpoints/ViT-B-32
logs:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/logs
device:
  desc: null
  value: cuda:0
saved_models:
  desc: null
  value: saved_models
model:
  desc: null
  value: ViT-B-32
seed:
  desc: null
  value: 1
iterations:
  desc: null
  value: 500
eval_iterations:
  desc: null
  value: 100
save_model_flag:
  desc: null
  value: true
pruned:
  desc: null
  value: false
wandb:
  desc: null
  value: true
wandb_logs:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/wandb
wandb_entity:
  desc: null
  value: osialm
wandb_project:
  desc: null
  value: intervmerge
name:
  desc: null
  value: experiment_name
group:
  desc: null
  value: lw_adamerging
train_lambdas:
  desc: null
  value: true
taskwise:
  desc: null
  value: false
layerwise:
  desc: null
  value: true
prior:
  desc: null
  value: 0.3
use_lambda_custom:
  desc: null
  value: true
use_intervention:
  desc: null
  value: true
train_intervention:
  desc: null
  value: true
reft_position:
  desc: null
  value: 0
low_rank_dimension:
  desc: null
  value: 1
num_layer:
  desc: null
  value: 12
start_layer:
  desc: null
  value: 11
config_type:
  desc: null
  value: one_token
intervention_type:
  desc: null
  value: default
model_hidden_size:
  desc: null
  value: 768
save_path:
  desc: null
  value: /home/marcin.osial/IntervMerge/src/saved_models
reft_config:
  desc: null
  value:
    SUN397:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    Cars:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    RESISC45:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    EuroSAT:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    SVHN:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    GTSRB:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    MNIST:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
    DTD:
      dropout: 0.0
      act_fn: linear
      localization:
      - layer: 11
        positions:
        - 0
        component: attention
      - layer: 10
        positions:
        - 0
        component: attention
      - layer: 9
        positions:
        - 0
        component: attention
      - layer: 8
        positions:
        - 0
        component: attention
      - layer: 7
        positions:
        - 0
        component: attention
      - layer: 6
        positions:
        - 0
        component: attention
      - layer: 5
        positions:
        - 0
        component: attention
      - layer: 4
        positions:
        - 0
        component: attention
      - layer: 3
        positions:
        - 0
        component: attention
      - layer: 2
        positions:
        - 0
        component: attention
      - layer: 1
        positions:
        - 0
        component: attention
      - layer: 0
        positions:
        - 0
        component: attention
custom_rlambdas:
  desc: null
  value: "[[0.3 0.3 0.3 ... 0.3 0.3 0.3]\n [0.3 0.3 0.3 ... 0.3 0.3 0.3]\n [0.3 0.3\
    \ 0.3 ... 0.3 0.3 0.3]\n ...\n [0.3 0.3 0.3 ... 0.3 0.3 0.3]\n [0.3 0.3 0.3 ...\
    \ 0.3 0.3 0.3]\n [0.3 0.3 0.3 ... 0.3 0.3 0.3]]"
logger:
  desc: null
  value: <Logger ConfigLogger_01-09-2024-09-28-50.log (DEBUG)>
_wandb:
  desc: null
  value:
    python_version: 3.8.18
    cli_version: 0.16.3
    framework: huggingface
    huggingface_version: 4.39.0
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1725182932.933949
    t:
      1:
      - 1
      - 5
      - 11
      - 41
      - 49
      - 51
      - 53
      - 55
      - 63
      2:
      - 1
      - 5
      - 11
      - 41
      - 49
      - 51
      - 53
      - 55
      - 63
      3:
      - 13
      - 16
      - 23
      4: 3.8.18
      5: 0.16.3
      6: 4.39.0
      8:
      - 5
      13: linux-x86_64
